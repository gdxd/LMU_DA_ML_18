{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Challenge Example\n",
    "In this part we will look at the **[Higgs Boson ML Challenge](https://www.kaggle.com/c/Higgs-boson)** on Kaggle and attempt a solution using Boosted Decision Trees (BDTs), a popular method in experimental particle physics. BDTs are based on an ensamble of weak classifiers (decision trees), and boosting increases the weight of misclassified events. Tomorrow, I will leave it as an excercise to try to get a solution using a neural network. The data is available from **[CERN Open Data](http://opendata.cern.ch/record/328)**. More information about the data is available from the links, and in particular at **[Documentation](http://opendata.cern.ch/record/329/files/atlas-higgs-challenge-2014.pdf)**. The general idea is that we want to extract $H\\to\\tau\\tau$ signal from background. In particular, the selection requires one of the taus to decay into an electron or muon and two neutrinos, and the other into hadrons and a neutrino. The challenge is based on Monte Carlo events processed through the **[ATLAS detector](http://atlas.cern/)** simulation and reconstruction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the usual setup: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load training data\n",
    "df = pd.read_csv('data/atlas-higgs-challenge-2014-v2.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Label</th>\n",
       "      <th>KaggleSet</th>\n",
       "      <th>KaggleWeight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>...</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>s</td>\n",
       "      <td>t</td>\n",
       "      <td>0.002653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.681042</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>2.233584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>...</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "      <td>0.715742</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>2.347389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>1.660654</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>5.446378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.904263</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>6.245333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EventId  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h  \\\n",
       "0   100000       138.470                       51.655        97.827    27.980   \n",
       "1   100001       160.937                       68.768       103.235    48.146   \n",
       "2   100002      -999.000                      162.172       125.953    35.635   \n",
       "3   100003       143.905                       81.417        80.943     0.414   \n",
       "4   100004       175.864                       16.915       134.805    16.405   \n",
       "\n",
       "   DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0                  0.91           124.711                2.666   \n",
       "1               -999.00          -999.000             -999.000   \n",
       "2               -999.00          -999.000             -999.000   \n",
       "3               -999.00          -999.000             -999.000   \n",
       "4               -999.00          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep  DER_pt_tot      ...       PRI_jet_leading_eta  \\\n",
       "0               3.064      41.928      ...                     2.150   \n",
       "1               3.473       2.078      ...                     0.725   \n",
       "2               3.148       9.336      ...                     2.053   \n",
       "3               3.310       0.414      ...                  -999.000   \n",
       "4               3.891      16.405      ...                  -999.000   \n",
       "\n",
       "   PRI_jet_leading_phi  PRI_jet_subleading_pt  PRI_jet_subleading_eta  \\\n",
       "0                0.444                 46.062                    1.24   \n",
       "1                1.158               -999.000                 -999.00   \n",
       "2               -2.028               -999.000                 -999.00   \n",
       "3             -999.000               -999.000                 -999.00   \n",
       "4             -999.000               -999.000                 -999.00   \n",
       "\n",
       "   PRI_jet_subleading_phi  PRI_jet_all_pt    Weight  Label  KaggleSet  \\\n",
       "0                  -2.475         113.497  0.000814      s          t   \n",
       "1                -999.000          46.226  0.681042      b          t   \n",
       "2                -999.000          44.251  0.715742      b          t   \n",
       "3                -999.000          -0.000  1.660654      b          t   \n",
       "4                -999.000           0.000  1.904263      b          t   \n",
       "\n",
       "   KaggleWeight  \n",
       "0      0.002653  \n",
       "1      2.233584  \n",
       "2      2.347389  \n",
       "3      5.446378  \n",
       "4      6.245333  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more information about the variables in the documentation. The variables that start with **DER** are derived quantities, determined by the physicists performing the analysis as variables that discriminate signal from backround. On the other hand, those that start with **PRI** are considered to be primary variables, from which the derived variables are calculated. They themselves generally do not provide much discrimination, but one if the ideas suggested by deep networks is that they can determine the necessary features from the primary variables, potentially even finding variables that the physicists did not consider. *EventId* identifies the event but is not a \"feature.\" The *Weight* is the event weight so that the sum of weights of all signal events should produce the signal yield expected to be observed in 2012, and the sum of weights of all background events should produce the backgroudn yield. Note that the weight varies event to event, because different background and signal processes contribute to the background and signal sets. *Label* indicates if it is a signal or background event. Ignore the *Kaggle* variables--they are only used if you want to reproduce exactly what was used in the Challenge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map y values to integers\n",
    "df['Label'] = df['Label'].map({'b':0, 's':1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Label</th>\n",
       "      <th>KaggleSet</th>\n",
       "      <th>KaggleWeight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>...</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>1</td>\n",
       "      <td>t</td>\n",
       "      <td>0.002653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.681042</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>2.233584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>...</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "      <td>0.715742</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>2.347389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>1.660654</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>5.446378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.904263</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>6.245333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EventId  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h  \\\n",
       "0   100000       138.470                       51.655        97.827    27.980   \n",
       "1   100001       160.937                       68.768       103.235    48.146   \n",
       "2   100002      -999.000                      162.172       125.953    35.635   \n",
       "3   100003       143.905                       81.417        80.943     0.414   \n",
       "4   100004       175.864                       16.915       134.805    16.405   \n",
       "\n",
       "   DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0                  0.91           124.711                2.666   \n",
       "1               -999.00          -999.000             -999.000   \n",
       "2               -999.00          -999.000             -999.000   \n",
       "3               -999.00          -999.000             -999.000   \n",
       "4               -999.00          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep  DER_pt_tot      ...       PRI_jet_leading_eta  \\\n",
       "0               3.064      41.928      ...                     2.150   \n",
       "1               3.473       2.078      ...                     0.725   \n",
       "2               3.148       9.336      ...                     2.053   \n",
       "3               3.310       0.414      ...                  -999.000   \n",
       "4               3.891      16.405      ...                  -999.000   \n",
       "\n",
       "   PRI_jet_leading_phi  PRI_jet_subleading_pt  PRI_jet_subleading_eta  \\\n",
       "0                0.444                 46.062                    1.24   \n",
       "1                1.158               -999.000                 -999.00   \n",
       "2               -2.028               -999.000                 -999.00   \n",
       "3             -999.000               -999.000                 -999.00   \n",
       "4             -999.000               -999.000                 -999.00   \n",
       "\n",
       "   PRI_jet_subleading_phi  PRI_jet_all_pt    Weight  Label  KaggleSet  \\\n",
       "0                  -2.475         113.497  0.000814      1          t   \n",
       "1                -999.000          46.226  0.681042      0          t   \n",
       "2                -999.000          44.251  0.715742      0          t   \n",
       "3                -999.000          -0.000  1.660654      0          t   \n",
       "4                -999.000           0.000  1.904263      0          t   \n",
       "\n",
       "   KaggleWeight  \n",
       "0      0.002653  \n",
       "1      2.233584  \n",
       "2      2.347389  \n",
       "3      5.446378  \n",
       "4      6.245333  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's create separate arrays\n",
    "eventID = df['EventId']\n",
    "X = df.loc[:,'DER_mass_MMC':'PRI_jet_all_pt']\n",
    "y = df['Label']\n",
    "weight = df['Weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now split into testing and training samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, eventID_train, event_ID_test, weight_train, weight_test = train_test_split(\n",
    "    X, y, eventID, weight, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now let's define the model\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators=50, max_depth=10,\n",
    "                                    min_samples_leaf=200,\n",
    "                                    max_features=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1989           17.06m\n",
      "         2           1.1310           17.93m\n",
      "         3           1.0749           17.60m\n",
      "         4           1.0268           17.13m\n",
      "         5           0.9876           16.74m\n",
      "         6           0.9519           16.46m\n",
      "         7           0.9225           15.89m\n",
      "         8           0.8963           15.44m\n",
      "         9           0.8736           15.12m\n",
      "        10           0.8540           14.73m\n",
      "        20           0.7457           10.87m\n",
      "        30           0.7052            6.90m\n",
      "        40           0.6854            3.29m\n",
      "        50           0.6727            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=10,\n",
       "              max_features=10, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=200,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=50, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and train\n",
    "gbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84204444872397866"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, let's look at logistic regression. This is a very simple, linear model. In the exercises you can look at optimizing it a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7511101070665398"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's try using fewer features\n",
    "lr2 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2.fit(X_train.loc[:,:'DER_pt_tot'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7168199274865843"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2.score(X_test.loc[:,:'DER_pt_tot'], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get back to the original problem using the GradientBoostingClassifier. The Kaggle competition used the approximate median segnificance (AMS), as defined below, to determine how good a solution was. The number 10, added to the background yield, is a regularization term to decrease the variance of the AMS.\n",
    "\n",
    "Note that if you do not use the full data set (i.e. you split into training and testing) you have to reweigh the inputs so that the subsample yield matches to the toal yield, which we will do below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute approximate median segnificance (AMS)\n",
    "\n",
    "def ams(s,b):\n",
    "    from math import sqrt,log\n",
    "    if b==0:\n",
    "        return 0\n",
    "\n",
    "    return sqrt(2*((s+b+10)*log(1+float(s)/(b+10))-s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's try a different probability cut, not the one given by default to predict().\n",
    "# We choose the top 15%, but can optimize\n",
    "y_train_prob = gbc.predict_proba(X_train)[:, 1]\n",
    "y_test_prob = gbc.predict_proba(X_test)[:, 1]\n",
    "pcut = np.percentile(y_train_prob,85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Prob']=gbc.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc3c2398c88>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgUAAAFeCAYAAAAVPWykAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3XecFdX9//HXllBXOkYBRVQ8lqhgiz3Yo2IvKIkt2HvU\nGBM1X8tP/WqKxAiWqLEramwx6FdjVIImGkWMATxgxN4oK7Dsgm75/XFnN+yyC3e5d7awr+fjsY9x\n77SzHy933vfMzJmCmpoaJEmSClu7AZIkqW0wFEiSJMBQIEmSEoYCSZIEGAokSVLCUCBJkgBDgSRJ\nShgKJEkSAMWt3YDmmDNnUc4jLRUWFtCnT3fmz19MdbUDN+WLdU2HdU2PtU2HdU1PrrXt33+NgpXu\nY5Va1o4VFhZQUFBAYeFKa6NmsK7psK7psbbpsK7paYnadrhQIEmSGmcokCRJgKFAkiQlDAWSJAkw\nFEiSpIShQJIkAYYCSZKUMBRIkiTAUCBJkhI5D3McQpgA7BRjHNTIvL7AL4ADgLWBD4A7gN/EGCtz\n3bckScqfnHoKQgi/AI5sYl4v4CXgLGAK8FugHLgWeDCX/UqSpPxbpVAQQugSQvg9cPkKFvsFsBlw\nRozx8BjjRcC2wKPAYSGEQ1dl35IkKR3NDgUhhAOAGcCJwMQmlukKnAx8BNxS+3qMsQq4IPn11Obu\nW5IkpWdVrikYA6wBnA7cDFQ3ssx2QHfg0RhjvfkxxtkhhNnAriGEoiQotJqps+Zmveywof1SbIkk\nSa1rVU4fjAWGxBhvijE29UDnjZLpu03Mfw/oDAxZhf1LkqQUNLunIMb4YhaL9U2m85uYvyCZ9mrO\nvgsLc3+OdFFRYYNp9tsrLvYOzqY0rKvyw7qmx9qmw7qmpyVqm/MtiU3olEyXNjG/9vUuzdlonz7d\nKSjILRTU6tGjKwAlJYuyXqd37+552ffqrLauyi/rmh5rmw7rmp40a5tWKKhIpp2amN85mZY1Z6Pz\n5y/OS09Bjx5dWbiwgqqqasrKlmS9bmnp4pz2vTprWFflh3VNz1vvzqVbt86Uly+lurqpM6Gtb/hG\n/Vu7Cc3iezY9udY2my+2aYWC2tMGTZ0e6JlMFzQxv1HV1TV5+8dbVVVNZWU1VVXZb6+y0jf4ytTW\nVfllXfOv9rOkurqmWZ8DLS1f/99vv/0W/vCH3y/3eqdOnenbty9bbbUNxx77IwYOXG4culWyovfs\nVVddxtNPP8X1149j222/m5f9rS5OO20Mb7/9Fg8//CRrrz2g0WXS/DxIKxS8k0w3aGL+BsBi4MOU\n9i9JasSwYVsxfPjWdb8vWbKEDz98n2ee+TMvvfRXbrnlTgYPXq/1GqhWlVYoeANYBIwIIRQue1ti\nCGF9YD3gL619O6IkdTTDh2/NmDGnLPf6c889w+WXX8L48b/l2muvb4WWqS1I5RLGGOMS4H4ytxye\nXft6CKEI+GXy67g09i1Jar4999yH7t27M2XK663dFLWitHoKAC4B9gGuDyHsDkwH9gK2Ah4Cnkhx\n35KkZigoKKCwsIji4vqHhRjfYcKE+5g6dQqlpfMpLi5m0KB12Hvv/Rg1ajSFhfW/W77zznQeeuh+\nXn31NRYvLmOttQaw997fZ9So0XTu3PQNZwsXLuTcc09j5szIySefzrHH/qhu3jPP/JmHH36QDz98\nn65duzFixO4cfPDhHHvsKPbddyQXX3wZ8N9rFcaPv43f/e43vPvuLPr27ccNN9zMgAEDWbp0CQ88\ncC9/+cuzfPrpx3Tq1IkQNmXUqNHsuOPO9dqz887b0L//mjz2WP2Bez/++COOOuoQhg3bihtvvBWA\niRP/xNVXX86VV/4v5eXlPPLIg3zwwQd07dqFbbfdnpNPPp0BAwbW2878+fO4447f8/LLk1iwYAFD\nh27EKaeckd3/rBSlFgpijHNDCDsCVwIjyQSC2cCFwG9XMPCRJKmFvfDCX1i0aCGHHnpE3WuvvfYP\nLrzwXLp06cquu46gT5++zJnzBZMmvci4cWMpLZ3H6aefU7f8Sy+9wGWX/Zyamhp22eV7rLXWAN58\n8w1uvXU8//rXVK699nqKioqW2/fixWWcf/6ZzJwZOe20s/jBD46rm3fLLeO4554/0LdvP/bZZz9q\namp49tmnefXVvzf5t1xyyU9Zd93BHH74UXz22acMGDCQsrIyzjrrZGbNmsmQIetz4IGHsGjRQiZP\nnsSFF57LiSeeyvHHn5hTDe+//25mzozssssItt12e95883X+8pf/41//msq99z5Mt27dACgtnc8p\np/yIzz77hOHDt2b33fdk+vRpnHfemZSUlOTUhlzlHApijE3eIxhj/IzMMxIkSW3Am2++we231z2S\nhq+//poPP3yfV16ZzNZbb8tpp9Wd8eXGG6+noKCAW2/9A+uuu17d67Nnv8exx45i4sSn6kJBefli\nrrvu/1FcXMydd97J4MFDqayspqamhosvvpBJk17g5Zf/xq67jqjXnvLyci644GxmzJjOmWeey1FH\n/bBu3qxZkfvuu4t11lmX8eNvo3fvPgCMHn0sY8b8kKYMHDiQG264uV4vxs03/45Zs2ZywAEHc/75\nF9X1iHzyycecddYp3HbbzWy11TZsscWw5he1rr0zGTfuNr7znc0BqK6u5pxzTuPNN99g8uSX2Hvv\nfQG49dbxfPbZJ5x00mkcd9yYZdp4I/fee+cq7z8f0jx9IElqY6ZOncLUqVMandezZy9KS+fTtetA\nampqOOmk06isrKwXCACGDFmfPn36MG/evLrX/v73l1mwYAGjRh3NsGHD6sZ1KSgo4KSTTkvW6Vtv\nO0uXLuHCC8/l7bf/xdlnn8+RRx5db/7EiU9RXV3NCSecXBcIAAYOHMSoUT+oF26Wtdtue9YLBN98\n8w3/938TKSlZg3PPvaDeKZKBAwdx0kmncdVVl/Hkk4/lFAq22277ukAAUFhYyM4778qbb77Bp59+\nAkBlZSXPP/8cffr05Yc/PL7e+ieeeCp//vOTlJY2NRhw+gwFktSBnHDCSfXuPli6dClz587h+eef\n5bbbbuatt6Zwxx330bdvP3bZZQQA8+bN5b333uWTTz7ho48+5J13plNaWgpAVVUVRUVFzJwZAdh8\n8y2X2+eQIetz0kmnLff6r399LXPmfEmnTp3Yfvsdlps/Y8Y0gHoH2lpbbjm8yb+x4fn7jz76gIqK\nCrbbbodGr2uo3dasWbHJbWajYXgCKClZA8gEE8hck1Bevpgttxy23KmU4uJiNt10M15++W85tSMX\nDk4tSR1Y586dGThwEMce+yOOOOIo5s2bxyOPTADggw/e5yc/OYeDD96XH//4TH71q2uYNOkF1lpr\nbbp3z5z7rqnJXB62cGFmLLrmnBOfM+dLdt11N77++muuueZKqqvrD8jz1VdfAdC37/JPqO3Xr+mR\nHhse+MvKylbYtv791wSgoqKi0fnZ6tRp+UF8a4fmr63TokULAejWrfHRBXv06Nno6y3FngJJEgDb\nbLMdEybcz7vvzqSiooJzzjmN0tL5HHPMCeyyy/cYPHi9uoPZgQfuU2/drl0z4/HXHoAbqqioqFum\n1imnnMExx5zAeeedyWuv/YNHHplQ7xRC7YV5ixeX0blz53rrlpeXZ/131QaYOXO+bHR+7YG6Z8/6\ng/A2DCm1f0cuevbMHPSbrlP2f1ca7CmQJAGwYEHtt/01eP31V5k7dw77738gJ598OptsslldICgt\nLeWrr0rrrbvhhhsBMG3av5fb7nvvvctee+3CNddcUe/1jTfeFIALLvgZXbp04fe/H88nn3xcN3+T\nTTLz//3vt5fb5rRpy7/WlMGD16NLly785z/vsnDhwuXm147NsOGGQ+te+9a3vtVo8Pj449wG4h0w\nYBA9evRkxoxpdacUatXU1DBjxvSctp8rQ4EkiYqKCh566AEAdt11RF0X/BdffFFvuaVLl3LddVfV\nfYuurKxM1tmN7t278+STjzNjxoy65WtqarjrrtuBzIV4jRkwYCBjxpxKRUUF1113VV1X+8iRB1FQ\nUMAdd9xadyoB4PPPP+O+++7K+m8rLi5m7733pbx8MTfc8Ou6NgN89tmn3HrreAoKCthvvwPqXh88\neAgVFeX1BnOqqKjggQfuzXq/TbVl331HsnDhAm6++ca6vxXggQfu4YsvPs9p+7ny9IEkdSANb0mE\nzEA6L730Al99Vcouu3yPESP2YOnSpQwatC6vvvoKZ5xxEt/5zhaUlS3i739/mXnz5tKrVy+++uor\nFiz4ii5d1qKkpISf/vRSLr/8EkaNGsWuu46gb9/+TJnyT2bOjIwYsQd77LF3k+068sijee65Z3jj\njX/yxBOPcvDBh7HJJptx5JGjmTDhPo4//mh23nlXvvnmGyZNerHuIr2Ggyc15fTTz2HatLd55pk/\nE+MMtt56WxYtWsTLL0+irKyME088td5FkoceegTXXXcVF110PnvttQ+dO3dm0qQX6d2793KnQZpr\nzJhTeOONfzJhwn28/fZbbL75lrz77izefPN1BgwYWHenQmswFEjqkIZv1J/evbtTWrq4Qz2BsuEt\niUVFRZSUlLD++huy55771H0779KlC2PHjuOWW8YxdeoUZsyYTr9+/dh4400ZPfoYXn/9NW677WZe\neWUyhxxyOAC7774nAwasxX333cWrr/6d8vJy1l57AKeccgZHH33MCttVVFTERRddwkknHcdNN93A\nDjvsxLe/vRZnnnkugwatw+OPP8LEiX+ipGQN9ttvJJtsshmXXXZx1gfokpISbrrpDh544B7++tfn\nePLJx+jatSvf+c4WjBo1mm23rd+LceCBh1BdXc2jjz7ExIl/omfPXowYsQcnnXQqhx02splVr69b\nt26MG3crd911O3/961947LGHWXfd9bjqqut44YXnWzUUFCzbddHWzZmzKOfGFhcX1vsgmDprbtbr\nDhu6/BWwymhYV+WHdU2PtU1HPus6b95cioqK6dWr13LznnzyMa677ipOP/1sRo8+Nqf9tBe51rZ/\n/zWaHGywltcUSJLapOeff46RI/dc7jx+eXk5jz76MABbb71dazRtteXpA0lSm7Tnnntz9913cPPN\nv2Pq1DcYMmQDysrKeOWVv/Hll19wxBFHE8LGrd3M1YqhQJLUJvXp05fbbruHBx+8h3/84xVef/01\nOnXqzPrrb8App5zBPvvs19pNXO0YCiRJbdZaa63Fuef+pLWb0WF4TYEkSQIMBZIkKWEokCRJgKFA\nkiQlDAWSJAkwFEiSpIShQJIkAYYCSZKUMBRIkiTAEQ0ldVBvfTmNkvIulJUtobq67T4tdvN+m+Z9\nm++8M52HH36QqVOnUFo6n06dOjNgwEB22GEnjjzyaHr2/O9TCc8882SmTp3Cgw8+xqBB6+S9LbmY\nMuV1zj77VPbee19+8YsrW7s5qwVDgSR1II899gi/+c21dO3ajR133Jk11/w25eXlvPPOdO6663Ye\nf/wRxo69iaFDNwJgv/0OYPjwrenRo0crt1wtwVAgSR3E559/xtixv2TddQczfvxt9XoEAP74x4e4\n/vrruOKKS7j77gkUFBSw334HtFJr1Rq8pkCSOoiXX/4bVVVV7L//QcsFAoDDDjuSjTbamNmz32P2\n7P+0QgvV2uwpkKQOorLyGwDefXdmk8ucd95PWbDgK/r3/zbQ9DUFzzzzZx5++EE+/PB9unbtxogR\nu3PwwYdz7LGjOOSQQ/jpTy8F4KqrLuPpp5/ij398iieeeJTnnvs/5s79kr59+7H77ntxwgkn0bVr\n13ptmDLldR55ZALTpr3NggVf0blzZ9Zbb30OOuhQey5SZiiQpA5iu+12oKBgLM8++zTl5YvZf/8D\n2XrrbenWrXvdMt/5zuYr3c4tt4zjnnv+QN++/dhnn/2oqanh2Wef5tVX/97kOpdcciEff/wxI0bs\nTrdu3Xjhhee5//67+eijD7nmml/VLffUU09w7bX/jz59+rDzzt+jpGQNPvnkYyZPfomrr76cJUuW\ncOihR+RWCDXJUCBJHcSQIetz1lnnMW7cWCZPnsTkyZMoKipigw2GMnz4Vuy44y4MH741hYVNn1me\nNSty3313sc466zJ+/G307t0HgNGjj2XMmB82ud6CBQu4//5H6pY/5pgfMXr0Yfztby8yZ86X9O+/\nJpWVlYwffwM9e/bizjsfqFsW4B//eIULLjibp5/+k6EgRV5TIEkdyJFHHs3vf38X3//+/qyxRg+q\nqqqYOfMdJky4n3POOY0TTvgB77wzvcn1J058iurqak444eR6B+2BAwcxatQPmlzv4IMPr7d8r169\n2GKLLQH49NNPAaiqquQnP/kZl156Rb1lAbbaahsASktLm/9HK2v2FEhSB7PRRhtzySWXU1VVxaxZ\nkSlT3uD111/ljTf+yX/+M4tzzz2d22+/l4EDBy237owZ04DGTzNsueXwJvc5ePB6y71WUrIGAN98\n8zUAnTt3Ybfd9gQyd0rMnv0fPv30Uz766AP+/e+3Aaiurm7eH6tmMRRIUgdVVFTExhtvysYbb8ro\n0cfw2WefcumlFyWDGz3Auef+ZLl1vvrqKwD69u233Lx+/fo3ua9OnTot91pBQQEANTX/HTzq3//+\nF+PG/Za3336rbplBg9Zh+PCteeed6fWWVf4ZCiSpA6iqquK4444C4N57H250mbXXHsA555zPaaeN\n4YMP3m90mW7dugGweHEZnTt3rjevvLw8pzZ+8cXn/PjHZ1JTU80ZZ5zLNttsx7rrrkvnzl1YunQp\nTz75WE7b18p5TYEkdQBFRUXU1NTw/vuz67riG1P77b1//zUbnb/JJplhlxvbxrRpTW83Gy+99AIV\nFeUcd9wYjj76hwwduhGdO3cBYPbs9wDsKUiZoUCSOoijj87cHXD55Rcza1Zcbv6iRYu46abfATBy\n5EGNbmPkyIMoKCjgjjturTuVAJlrAO67766c2lfb8/D555/Ve33hwoWMHftLACorK3Pah1bM0weS\n1EGMHHkws2fPZsKE+xgz5hi22GIYG20U6Ny5C59++gmvvvoK5eXlnH762WyxxbBGt7HJJptx5JGj\nmTDhPo4//mh23nlXvvnmGyZNepGioiKAFd7SuCI77bQrt946jieeeJQvv/yCDTYYyvz583j55UlU\nVCyhe/fuLF5cRmVlJcXFHr7SYFUldUhbrrkZvXt3p7R0MZWVHeeK9rPO+jEjRuzOU089wVtvTWXm\nzMg333xNnz592WmnXTnssCPZdNPvrHAbZ555LoMGrcPjjz/CxIl/oqRkDfbbbySbbLIZl112cd11\nB83Vr18/brjhFm69dRzTp0/jzTffoH//Ndl++x354Q9PYMKE+3jqqSf45z9fZYcddlqlfWjFCtrT\n+Zk5cxbl3Nji4sJ6HwRTZ83Net1hQ5e/2lYZDeuq/LCu6bG2q2bevLkUFRXTq9fyz0548snHuO66\nq/jJT37CYYcdbV3zLNf3bP/+axSsbBmvKZAkZe35559j5Mg9eeCBe+u9Xl5ezqOPZu5q2GGHHVqj\nacoDTx9IkrK25557c/fdd3Dzzb9j6tQ3GDJkA8rKynjllb/x5ZdfMGrU0Wy22WaUli5u7aZqFRgK\nJElZ69OnL7fddg8PPngP//jHK7z++mt06tSZ9dffgFNOOYP99x/Z2k1UDlIPBSGEYuAC4DhgfaAc\neAW4Isb4atr7lyTl11prrdXoaIdq/1rimoKHgWvIBJDxwFPAXsDfQgh7t8D+JUlSFlLtKQgh7AUc\nDLwO7BJjXJK8fgfwFzIhYcM02yBJkrKTdk/Bd5PpPbWBACDG+ALwDrBBCKHxsTQlSVKLSjsU1A4C\nsN6yL4YQOgH9gG+ABSm3QZIkZSHtCw0fBi4HTg8hvAU8BvQCrgXWBH4VY1ya7cYKCwsoLFzp2Asr\nVFRU2GCa/faKix3WoSkN66r8sK7psbbpsK7paYnapj6iYQhhA+BOYOcGsy4GrokxZt2Ampqamton\neOXLa9M+z3rZ7TZbK6/7liSpBa30AJr2hYadgUuBHYEpwCSgD3AI8DPgEyDrx2rNn784Lz0FPXp0\nZeHCCqqqqikrW7LylRIOxtG0hnVVfljX9FjbdFjX9ORa2969u690mbRPH/yKzPgEvwV+XNsrEEK4\nFJgM/CGEMD3G+M9sNlZdXUN1dX56NqqqqqmsrKaqKvvtOY73ytXWVfllXdNjbdNhXdOTZm1TOzER\nQigETiRzIeGFy54miDF+SOb0QUGyjCRJamVpXgmyJtAF+E+M8etG5r+dTAen2AZJkpSlNENBKbAU\nGJLcgtjQRsn0sxTbIEmSspRaKEhuNXwU6A1cuey8EEL/ZV67F0mS1OrSvtDwx8A2wIUhhN2BF8nc\nfXAQ0Bf4dYzx+ZTbIEmSspDq6BIxxi+AbckMVtQTOBs4ApgGHBFjvCDN/UuSpOyl/ujkGOMC4KLk\nR5IktVGOQylJkgBDgSRJShgKJEkSYCiQJEkJQ4EkSQIMBZIkKWEokCRJgKFAkiQlDAWSJAkwFEiS\npIShQJIkAYYCSZKUMBRIkiTAUCBJkhKGAkmSBBgKJElSwlAgSZIAQ4EkSUoYCiRJEmAokCRJCUOB\nJEkCDAWSJClhKJAkSYChQJIkJQwFkiQJMBRIkqSEoUCSJAGGAkmSlDAUSJIkwFAgSZIShgJJkgRA\ncWs3oD2ZOmtuVssNG9ov5ZZIkpR/9hRIkiTAUCBJkhKGAkmSBBgKJElSwlAgSZIAQ4EkSUoYCiRJ\nEmAokCRJCUOBJEkCWmhEwxDCvsD5wLZADTADuD7G+FBL7F+SJK1c6j0FIYQfAxOBzYF7gPuBDYAJ\nIYTz096/JEnKTqqhIISwOXAdmZ6BzWOMZ8YYTycTEL4Argkh9EyzDZIkKTtp9xScTeYUxakxxi9r\nX4wxfgH8HPgD8O2U2yBJkrKQ9jUF+wOfxRgnNZwRY7wDuCPl/UuSpCylFgpCCP2BtYHnQghrA1cC\nI4GewNvA1THGx5uzzcLCAgoLC3JqV1FRYYNpbttrTHFxx7upo2FdlR/WNT3WNh3WNT0tUduCmpqa\nVDYcQtgSmAq8CgwGyslccNgDOBQoAc6OMf4u223W1NTUFBTk9yD+2rTP87o9gO02Wyvv25QkKUcr\nPYCmGQp2AiYnv/4VODDGuDiZtzHwGtAFGBpj/CCbbc6bV1aTj56CHj26snBhBVVV1bw5c05O22vM\n8I36532bbV3Duio/rGt6rG06rGt6cq1t797dV3oATfOagqpl/vus2kAAEGN8J4TwOzIXGx4O/Dqb\nDVZX11BdnZ8QU1VVTWVlNVVV+Q9FlZUd9x9CbV2VX9Y1PdY2HdY1PWnWNs2TPguS6WIytyQ2NCWZ\nbphiGyRJUpbSDAX/ASrJ9EY01mXRKZmWp9gGSZKUpdRCQYzxa+DvQGdg10YW2TaZvpVWGyRJUvbS\nvmdkfDL9zbIjF4YQtgBOBeYBj6XcBkmSlIVUBy+KMT4YQtgHOB6YFkL4I5lxCo5I9n1ijHFRmm2Q\nJEnZaYnRJX6U/HwKnAgcCLwEjGju4EWSJCk9qT86OcZYQ+YZB39Ie1+SJGnVOQ6lJEkCDAWSJClh\nKJAkSYChQJIkJQwFkiQJMBRIkqSEoUCSJAGGAkmSlDAUSJIkwFAgSZIShgJJkgQYCiRJUsJQIEmS\nAEOBJElKGAokSRJgKJAkSQlDgSRJAgwFkiQpYSiQJEmAoUCSJCUMBZIkCTAUSJKkhKFAkiQBhgJJ\nkpQwFEiSJMBQIEmSEoYCSZIEGAokSVLCUCBJkgBDgSRJShgKJEkSYCiQJEkJQ4EkSQIMBZIkKWEo\nkCRJgKFAkiQlDAWSJAkwFEiSpIShQJIkAYYCSZKUMBRIkiSgFUJBCGG3EEJ1COHOlt63JElqWouG\nghBCD+APQEFL7leSJK1cS/cUjAUGt/A+JUlSFlosFIQQDgBOAJ5sqX1KkqTstUgoCCH0A34PvATc\n2BL7lCRJzVPcQvu5CSgh01OwwapupLCwgMLC3C5HKCoqbDDN/+UNxcUd76aOhnVVfljX9FjbdFjX\n9LREbVMPBSGEHwCHA2fEGGeHEFY5FPTp052CgvwcxHv06ApAScmivGxvWb17d8/7NtuL2roqv6xr\neqxtOqxretKsbaqhIIQwkMzpgufJ9BbkZP78xXnpKejRoysLF1ZQVVVNWdmSXJu1nNLSxXnfZlvX\nsK7KD+uaHmubDuuanlxrm80X1rR7Cm4HioAxMcaaXDdWXV1DdXXOmwGgqqqayspqqqrys71lVVZ2\n3H8ItXVVflnX9FjbdFjX9KRZ29RCQQjhVGAf4JQY4wdp7actmjprbtbLDhvaL8WWSJKUvTR7Co5K\npreEEG5pZP5xIYTjgLtijMen2A5JkpSFNEPBncCLjby+PnAM8BbwODA1xTZIkqQspRYKYox3NvZ6\nCGFPMqFgaozxsrT2L0mSmscbSSVJEmAokCRJiZYa0bBOjPEv+JRESZLaHHsKJEkSYCiQJEkJQ4Ek\nSQIMBZIkKWEokCRJgKFAkiQlDAWSJAkwFEiSpIShQJIkAYYCSZKUMBRIkiTAUCBJkhKGAkmSBBgK\nJElSwlAgSZIAQ4EkSUoYCiRJEmAokCRJCUOBJEkCoLi1GyBJUlvz9tzpWS+7eb9NU2xJy7KnQJIk\nAYYCSZKU8PSBJKnDaM5pgY7IngJJkgQYCiRJUsLTB61s6qy5WS03bGi/lFsiSero7CmQJEmAoUCS\nJCU8fSBJUg6yvaOhPQxyZE+BJEkCDAWSJCnh6QNJUrvmgET5Y0+BJEkCDAWSJClhKJAkSYChQJIk\nJQwFkiQJMBRIkqSEoUCSJAGGAkmSlDAUSJIkoAVGNAwhlAA/Bw4F1gO+Bt4ExsYYH0t7/5IktQXN\nGXmxtR6elGooCCGsAUwGtgCmAOOBnsBhwKMhhJ/HGK9Jsw2SpPbJ4YtbXtqnD35KJhDcDGwTYzwv\nxjgG2Az4DLgyhLBhym2QJElZSPv0wSigBvhZjLGm9sUY4ychhJuAK4D9gBtSbke7N3XW3KyXHTa0\nX4otkSQSNAOiAAAN+UlEQVStrtIOBWOBnjHGrxqZtzSZrpFyGyRJUhZSDQUxxnGNvR5CKCBzXQHA\nv7LdXmFhAYWFBTm1qaiosME0t+21RcXFLX9TScO6Kj+sa3qsbTryWddcP+/bs8Y+x1viPZv63QdN\nOA3YDngPeCbblfr06U5BQX7eJD16dAWgpGRRXrbXlvTu3b3V9l1bV+WXdU2PtU1HPupaUt4lDy1p\nn1b0OZ7me7bFQ0EI4Ugy1xBUAsfFGL/Jdt358xfnpaegR4+uLFxYQVVVNWVlS3LaXltUWrq4xffZ\nsK7KD+uaHmubjnzWdXX8fM5WY5/judY2my+MLRoKQginATeSufjw2Bjj5OasX11dQ3V1zcoXzEJV\nVTWVldVUVeVne21JZWXrfcDV1lX5ZV3TY23TkY+65uvzvj1aUe3SfM+2SCgIIRQCvwTOI3OB4egY\n46MtsW9JkpSdlhjRsBPwAJkRDecDBzW3h0CSJKUv7RENi4CHgQOB2cC+McaY5j4lSW2XoxS2bWn3\nFFxEJhB8COwSY/wk5f1JkqRVlFooCCH0AX6W/PomcFIIobFFJ8UY/5pWOyRJUnbS7CnYBai9/+Gg\n5KcxVwGGAkmSWllqoSDG+ATQcYejakXZPifBZyRIkpbl+J6SJAkwFEiSpERrPftAkrQaqb3VsLCw\ngJLyLpSVLenQIxK2V/YUSJIkwFAgSZIShgJJkgQYCiRJUsILDTswxzOQtCI+p6DjsadAkiQBhgJJ\nkpQwFEiSJMBrCiSpw/FaATXFngJJkgTYU6AsZHOXQlFRASUlixg6YI0WaJEkKQ2GAklaDXhKQPng\n6QNJkgQYCiRJUsLTB8qrN2fOoapq5Y9LdZRESWp7DAWS1IZ5rYBakqFAklqYB3q1VV5TIEmSAHsK\n1EqyfUIjeP2BJLUUQ4Ek5YmnBdTeGQokaQU80KsjMRSozcv2VIOnGdQcb305jZLyLpSVLaG6euW3\n0UodgRcaSpIkwJ4CrUa8eHH1lUYXfmFhQd63KbV3hgJJeeU5eKn9MhSoQ/I6BQ/ekpZnKJDyLJ8H\n28LCAi+Gk9RiDAXq0D6oeHfF8/+14vnLWn9Aj1ybI0mtylCgdmNlB/DW9t6nC7NazvAgqa0yFKhZ\nmjowFxYW0LX6W1Qs+cZubklqpwwFq7G2/s26o8q2RwFgw0E9U2yJJNVnKGgjPICrMe99upCuXSpW\n2gPjKQlJ+WAokFYDzel9MEBIaoqhIEV++1db5AWRkppiKFgFHuzVEdj7IHU8hgJJOWtOgMiWQUNq\neYYCSW2SQUNqeYYCSR3GskGjsLAgqzs78sVAovagRUJBCOFY4BwgAOXAs8DFMcYPWmL/2fA6AUlp\nSqPnIw2Gl44t9VAQQrgK+DkwHRgHrAscBewTQtguxjg77TZIkrKTa3hprAfGoNF+pBoKQghbkgkE\nk4E9YoxfJ68/BDwK/BY4MM02SJJaV2v3khhKspd2T8HZyfSK2kAAEGN8LIQwCRgZQhgYY/wk5XZI\nkjoox+bIXtqhYHegEpjUyLzngV2B3YB7U26HJEkrZHhIMRSEEDoBg4H3Y4xLG1nkvWS6cbbbLCws\noLCwIKd2FRUVNpgW1G1bq662fIUFy/yinFnX9FjbdHSEur7/+aKsl13VAFFcXLjcaw2PX2lIs6eg\nD1AAzG9i/oJk2ivbDfbtW5K3d1iPHl0B2P273ZNX1svXpiVJSk3t8SsN6cUN6JRMG+slWPb1Lim2\nQZIkZSnNUFCRTDs1Mb9zMi1LsQ2SJClLaYaCBUA1TZ8e6LnMcpIkqZWlFgqSWxDfA9YNIXyrkUU2\nSKbT02qDJEnKXpo9BQAvkjl9sFMj8/YAaoCXU26DJEnKQtqh4I5kenUIoe5yyRDCIcAuwJMxxo9T\nboMkScpCQU1Nuk8HCyHcCJwBzAIeBwYBRwJzgR1jjO+tYHVJktRC0u4pADgr+VlKZtjj7wEPYiCQ\nJKlNSb2nQJIktQ8t0VMgSZLagbQfiNRiQgjHAucAASgHngUujjF+kOX66wJXkLkroi8wExgXY/x9\nOi1uH/JQ192AC4HvAiXAp8CfyDw5c04qjW4ncq1tg20VkHnI2G7AkBjj+3lsaruSh/dsb+Bi4FBg\nAJn37HPAZTHGz1JpdDuRh9oOAy4jc6H5GsD7wH3A/zbxjJwOJ4QwAdgpxjioGevk7fi1WvQUhBCu\nAu4iM2TyODIfjkcBr4cQhmSx/mDg78Bo4AXgRqA7cGsI4ddptbuty0Ndj+e/T8P8M3AD8DFwJvDP\nEMJa6bS87cu1to04m0wg6NDy8J79NvAP4HwyF0ffQGa8lZOBV0IIfVNqepuXh9puT+Zz9oBk3d+R\nudbsMmBiCKEonZa3HyGEX5C5EL856+T1+NXuewpCCFsCPwcmA3skgyYRQngIeBT4LXDgSjZzPZlv\nBPvHGCcm6/8P8FfgxyGE+2OMb6T0J7RJudY1+bZ1A5lhrLeNMcZl5l0BXApcCxyX1t/QVuXpPbvs\n9gJwTQpNbVfyVNcbgI2As2KMNy6z7f8hc/C6EPhp3hvfxuWptr8mEygOjzH+MVm/GJgI7AUcDdyb\nyh/QxoUQupAJSSeuwup5PX6tDj0FZyfTK2rfqAAxxseAScDIEMLAplZOUtbBwCu1BU3WryDzj6AA\nOCWNhrdxOdUV2I9M9+BtywaCxJVkviEckMf2tie51rZO8u3qbuBL4F/5bmg7k+tnwSDgCOCFZQNB\nYixwD9BRTx/k4z27HVBaGwiS9SuB2i7uHfPY3nYjhHAAMINMIJi4ksUbrpv349fqEAp2ByrJvDEb\nep5MUVbUrToiWeb5RuZNBr5O9tHR5FrXGWTOyz7SyLwq4Bsy1xh0RLnWdlk/A7YFxgDZP+R99ZRr\nXfdLlpnQcEaMcUGM8dgY49h8NLQdysd7dh7QI+lFXNaAZNpRrzEaQ+YL1OnAyGauO4I8H7/adSgI\nIXQCBgMfNXGRSu04CBuvYDMbJdN3G86IMX4DfAQMSfbVIeSjrjHGKTHGq2OMrzQy+/tkAsHbOTe2\nncnTe7Z2W8OBXwA3xxgb+1DoMPJU1y2T6b9DCD8IIbwWQigPIXweQrgphNAvn21uL/L4nh0HFAET\nQggbhxC6hxAOAv4HmA/cnq82tzNjyVwcfFOMsbljBOT9+NWuQwHQh0xKmt/E/NonMDb1pEbIXKnJ\nSrZRCPRoduvar3zUtVEhhJ5k/hEAjG9+09q9vNQ2hNCZzGmDj8mc5+7o8lHX2m+sF5Cp7SfAzcCH\nwKlkLjTsk3tT2528vGdjjFeSGcjue2R6EsvIjHL7GfDdGOOHeWltOxNjfDHGuKq9fHk/frX3UFCb\nfpq6laX29S4pb2N1k0pNQghrkLkLYSjwNP99NkZHkq/aXglsBpwQYyzLR8PauXzUtfZ01kHAATHG\nQ2KM55G5nfYWMu/b/821oe1QXt6zye3JF5E5DXEfmQvkXgU2BW7voIErV3n/rG7voaAimTbVNdI5\nma7oQzMf21jd5L0mye2HL5J5YuarwKhV6CpbHeRc2xDCzmRumbsxxvhSHtvWnuXjPVuVTB9pcNFW\nDZnemCXAkSGE9v652Vz5eM8OIvOFoAswLMb4wxjjeTHG7YHLydy2fHee2tuR5P2zur2/uRcA1TTd\nbdVzmeWaUtvtsqJt1AALm9269isfda0TQtgceA3YisxtMnvl0F3W3uVU2xBCd+BOMudxL8p349qx\nfLxna+f9s+GMGONCMudtewL9V7GN7VU+ansM0BX4ZYxxVoN5l5Op7f4hhLVzaWgHlPfjV7sOBcmt\nMe8B64YQvtXIIhsk0+kr2Mw7DZatk2xzncyuYnUubW1P8lRXAEIIu5O5CnYdMvcg79uBA0E+artt\nssyGwOIQQk3tD5leGIDZyWvr5bHpbVqe3rO1t8429a2r9vXy5rew/cpTbQc3tUzSEzOtwXLKTt6P\nX+06FCReJPOPdadG5u1BJiW9vIL1X0qWaey2jV2SbU/OrYnt0ovkVldCCLsAT5G5yOXqGOMxy97j\n3IG9yKrX9n0y36wa+/koWea3ye9f5avB7cSL5PaefTGZ7tVwRnLnwRBgdgcNtS+SW20/T6ahiflD\nk2lHHQdiVeX9+LU6hILai9WuDiF0rX0xhHAImaI8GWP8uKmVk3nPAt8LIRy8zPpdgauSX8flvdVt\nX051TYaDfYhMl+GlMcaL02xsO7PKtY0xvh9jvKyxHzJXyQOMTV7raKEgp/csmQPfdDKfBccss34h\nmdH4vsV/B9rpaHKt7UNkTkFc0HBI5BDC2WQuNpy8Ks/96MjSOH6tFo9ODiHcCJxBZqzyx4FBZMaP\nngvsGGN8L1luBJnBHqbGGB9fZv2NyIwd3ZPMm/djMqNEDSVzDqxD3vKVS11DCFeTGVjnKzLfXJty\nRUc6NVMr1/dsE9ucTOabXId9IFIePguGkxkIpheZ0eXeSZbbGngFGJHc/93h5KG255EJV2Vkhkb+\nkszpsO+R6UnYtZHrDTqc5FTgJw0fiNRSx6/VoacAMve+nkXm9ouzybzJHmSZN2piBJmBMg5eduUY\n40xge+CPwD5k3viLyQw72eHGOV9GLnXdN5n2SuY19bO6vAebK6f3rJqU62fBm8BwMhdzbkXm4V09\nyZyO2aOjBoJErrX9DbA3mdMMB5J52uJgMg/wGW4gWKkRtMDxa7XoKZAkSbnrqN/SJElSA4YCSZIE\nGAokSVLCUCBJkgBDgSRJShgKJEkSYCiQJEkJQ4EkSQIMBZIkKWEokCRJgKFAkiQlDAWSJAmA/w/h\nCcYfkVLtjQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc3c2493668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kwargs = dict(histtype='stepfilled', alpha=0.3, normed=True, bins=40)\n",
    "\n",
    "df[df.Label==0].Prob.hist(label='Background',**kwargs)\n",
    "df[df.Label==1].Prob.hist(label='Signal',**kwargs)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's calculate the total weights (yields)\n",
    "sigall = weight.dot(y)\n",
    "backall = weight.dot(y == 0)\n",
    "\n",
    "# The training weights\n",
    "sigtrain = weight_train.dot(y_train)\n",
    "backtrain = weight_train.dot(y_train == 0)\n",
    "\n",
    "# The training weights\n",
    "sigtest = weight_test.dot(y_test)\n",
    "backtest = weight_test.dot(y_test == 0)\n",
    "\n",
    "# aside:  these can also be done by looping instead of using a dot product\n",
    "#  (Usually vectorized operations are faster for interpreted code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's look at event yields that pass our selection\n",
    "sigtrain_sel = weight_train.dot(np.multiply(y_train, y_train_prob > pcut))\n",
    "backtrain_sel = weight_train.dot(np.multiply(y_train == 0, y_train_prob > pcut))\n",
    "\n",
    "sigtest_sel = weight_test.dot(np.multiply(y_test, y_test_prob > pcut))\n",
    "backtest_sel = weight_test.dot(np.multiply(y_test == 0, y_test_prob > pcut))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected selected yields in training sample, signal = 224.643985018 , background = 3123.65892697\n",
      "Corrected selected yields in test sample, signal = 220.851709539 , background = 3762.23140237\n"
     ]
    }
   ],
   "source": [
    "# Now we need to correct the selected yields to be is if we used the full sample\n",
    "sigtrain_sel_corr = sigtrain_sel*sigall/sigtrain\n",
    "backtrain_sel_corr = backtrain_sel*backall/backtrain\n",
    "\n",
    "sigtest_sel_corr = sigtest_sel*sigall/sigtest\n",
    "backtest_sel_corr = backtest_sel*backall/backtest\n",
    "\n",
    "print(\"Corrected selected yields in training sample, signal =\", sigtrain_sel_corr, \", background =\",backtrain_sel_corr)\n",
    "print(\"Corrected selected yields in test sample, signal =\", sigtest_sel_corr, \", background =\",backtest_sel_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMS of training sample 3.9664288000542305\n",
      "AMS of test sample 3.5615919708673096\n"
     ]
    }
   ],
   "source": [
    "print(\"AMS of training sample\", ams(sigtrain_sel_corr,backtrain_sel_corr))\n",
    "print(\"AMS of test sample\", ams(sigtest_sel_corr,backtest_sel_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do? Not too bad. Here are the scores of real submissions.\n",
    "![Comparison with submissions](data/tr150908_davidRousseau_TMVAFuture_HiggsML.001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is of course a bit of a simplification from a real physics analysis, where systematics often seem to take the most time. They are ignored here.\n",
    "![Comparison with real analysis](data/tr140415_davidRousseau_Rome_Higgs_MVA_HiggsML.001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to work on for the rest of the day:\n",
    "1. Attempt to calculate the AMS for the logistic regression cases.\n",
    "1. Do we overfit? Add plots to see.\n",
    "1. Which variables are important?\n",
    "1. Should we **[preprocess](http://scikit-learn.org/stable/modules/preprocessing.html)** the input data to be the same scale? Note that we have some -999 values that indicate the variable could not be calculated.\n",
    "1. We do not use the event weights in the training. Can they help? Note, that you don't want to just apply the weights as is since they will make background dominate over signal.\n",
    "1. The best scores in the Challenge all used cross-validation; if you have time, try to implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomorrow we will continue on this example with neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
